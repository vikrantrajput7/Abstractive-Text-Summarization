{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSMO Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading folders which contains Data\n",
    "fold=os.listdir(\"/media/vikrant/New Volume/1. MSMO Data Complete\") #Directory where data is stored\n",
    "fold1=[i for i in fold if 'data' in i and 'zip' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where all data is saved\n",
    "base=\"/media/vikrant/New Volume/1. MSMO Data Complete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14657/14657 [04:35<00:00, 53.15it/s]\n",
      "100%|██████████| 14600/14600 [03:48<00:00, 63.96it/s]\n",
      "100%|██████████| 15271/15271 [03:55<00:00, 65.13it/s]\n",
      "100%|██████████| 14956/14956 [03:48<00:00, 65.41it/s]\n",
      "100%|██████████| 15151/15151 [03:50<00:00, 65.70it/s]\n",
      "100%|██████████| 14486/14486 [03:38<00:00, 66.44it/s]\n",
      "100%|██████████| 15401/15401 [04:16<00:00, 60.14it/s]\n",
      "100%|██████████| 14792/14792 [04:18<00:00, 57.25it/s]\n",
      "100%|██████████| 15200/15200 [03:27<00:00, 73.38it/s]\n",
      "100%|██████████| 14394/14394 [03:20<00:00, 75.31it/s] \n",
      "100%|██████████| 14200/14200 [04:06<00:00, 57.55it/s]\n",
      "100%|██████████| 14861/14861 [05:50<00:00, 42.45it/s]\n",
      "100%|██████████| 11485/11485 [03:16<00:00, 58.41it/s]\n",
      "100%|██████████| 15000/15000 [04:51<00:00, 51.44it/s]\n",
      "100%|██████████| 15500/15500 [04:03<00:00, 65.37it/s]\n",
      "100%|██████████| 14507/14507 [03:51<00:00, 62.67it/s]\n",
      "100%|██████████| 14917/14917 [03:58<00:00, 62.56it/s]\n",
      "100%|██████████| 14857/14857 [04:02<00:00, 61.28it/s]\n",
      "100%|██████████| 14800/14800 [03:59<00:00, 61.77it/s]\n",
      "100%|██████████| 14931/14931 [04:03<00:00, 61.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating .txt files to save Data\n",
    "head=open('train_title.txt','w')          # This file will contain training article's title\n",
    "document=open('train_document.txt','w')   # This file will contain training text articles\n",
    "doc_name=open('train_doc_name.txt','w')  # This file will contain training article file's names\n",
    "# Extracting Data\n",
    "for main in fold1:  # Looping through every folder\n",
    "    art1=os.listdir(base + str(main) +'/article') # Getting article .txt file names\n",
    "    for file in tqdm(art1):                       # Looping through every text file\n",
    "        with open(base + main +'/article/'+file,'r') as article:\n",
    "            art=article.read()\n",
    "            x=re.sub(r'\\s',' ',art)\n",
    "            x=re.sub('\\@body','\\n@body',x)\n",
    "            x=re.sub('\\@summary','\\n@summary',x)\n",
    "            with open(\"data.txt\",'w') as f:\n",
    "                f.write(x)\n",
    "            with open(\"data.txt\",'r') as f:\n",
    "                doc=f.readlines()\n",
    "            summary=[]\n",
    "            for i in doc:\n",
    "                i=re.sub(r'\\s',' ',i)\n",
    "                if '@title' in i:\n",
    "                    title=i\n",
    "                if '@body' in i:\n",
    "                    body=i\n",
    "# Only extracting those articles which have minimum 75 words\n",
    "# And summary contains more than 8 words\n",
    "            if len(body.split())> 75 and len(title.split())> 8 :\n",
    "                head.write(re.sub('\\@title','',title)+\"\\n\")\n",
    "                document.write(re.sub('\\@body','',body)+\"\\n\")\n",
    "                doc_name.write(file+'\\n')\n",
    "# Closing Text Files\n",
    "head.close()\n",
    "document.close()\n",
    "doc_name.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold2=['valid_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10355/10355 [03:02<00:00, 56.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating .txt files to save Data\n",
    "head=open('val_title.txt','w')          # This file will contain validation article's title\n",
    "document=open('val_document.txt','w')   # This file will contain validation text articles\n",
    "doc_name=open('val_doc_name.txt','w')  # This file will contain validation article file's names\n",
    "# Extracting Data\n",
    "for main in fold2:  # Looping through every folder\n",
    "    art1=os.listdir(base + str(main) +'/article') # Getting article .txt file names\n",
    "    for file in tqdm(art1):                       # Looping through every text file\n",
    "        with open(base + main +'/article/'+file,'r') as article:\n",
    "            art=article.read()\n",
    "            x=re.sub(r'\\s',' ',art)\n",
    "            x=re.sub('\\@body','\\n@body',x)\n",
    "            x=re.sub('\\@summary','\\n@summary',x)\n",
    "            with open(\"data.txt\",'w') as f:\n",
    "                f.write(x)\n",
    "            with open(\"data.txt\",'r') as f:\n",
    "                doc=f.readlines()\n",
    "            summary=[]\n",
    "            for i in doc:\n",
    "                i=re.sub(r'\\s',' ',i)\n",
    "                if '@title' in i:\n",
    "                    title=i\n",
    "                if '@body' in i:\n",
    "                    body=i\n",
    "# Only extracting those articles which have minimum 75 words\n",
    "# And summary contains more than 8 words\n",
    "            if len(body.split())> 75 and len(title.split())> 8 :\n",
    "                head.write(re.sub('\\@title','',title)+\"\\n\")\n",
    "                document.write(re.sub('\\@body','',body)+\"\\n\")\n",
    "                doc_name.write(file+'\\n')\n",
    "# Closing Text Files\n",
    "head.close()\n",
    "document.close()\n",
    "doc_name.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold3=['test_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10261/10261 [04:17<00:00, 39.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating .txt files to save Data\n",
    "head=open('test_title.txt','w')          # This file will contain validation article's title\n",
    "document=open('test_document.txt','w')   # This file will contain validation text articles\n",
    "doc_name=open('test_doc_name.txt','w')  # This file will contain validation article file's names\n",
    "# Extracting Data\n",
    "for main in fold3:  # Looping through every folder\n",
    "    art1=os.listdir(base + str(main) +'/article') # Getting article .txt file names\n",
    "    for file in tqdm(art1):                       # Looping through every text file\n",
    "        with open(base + main +'/article/'+file,'r') as article:\n",
    "            art=article.read()\n",
    "            x=re.sub(r'\\s',' ',art)\n",
    "            x=re.sub('\\@body','\\n@body',x)\n",
    "            x=re.sub('\\@summary','\\n@summary',x)\n",
    "            with open(\"data.txt\",'w') as f:\n",
    "                f.write(x)\n",
    "            with open(\"data.txt\",'r') as f:\n",
    "                doc=f.readlines()\n",
    "            summary=[]\n",
    "            for i in doc:\n",
    "                i=re.sub(r'\\s',' ',i)\n",
    "                if '@title' in i:\n",
    "                    title=i\n",
    "                if '@body' in i:\n",
    "                    body=i\n",
    "# Only extracting those articles which have minimum 75 words\n",
    "# And summary contains more than 8 words\n",
    "            if len(body.split())> 75 and len(title.split())> 8 :\n",
    "                head.write(re.sub('\\@title','',title)+\"\\n\")\n",
    "                document.write(re.sub('\\@body','',body)+\"\\n\")\n",
    "                doc_name.write(file+'\\n')\n",
    "# Closing Text Files\n",
    "head.close()\n",
    "document.close()\n",
    "doc_name.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Train Documents :  293625\n",
      "No of Train Summary :  293625\n",
      "No of Train Files-name :  293625\n",
      "No of Val. Documents :  10339\n",
      "No of Val. Summary :  10339\n",
      "No of Val. Files-name :  10339\n",
      "No of test Documents :  10245\n",
      "No of test Summary :  10245\n",
      "No of test Files-name :  10245\n"
     ]
    }
   ],
   "source": [
    "# Training Data Statistics\n",
    "document=open('train_document.txt','r')\n",
    "summ=open('train_title.txt','r')\n",
    "file_name=open('train_doc_name.txt','r')\n",
    "print(\"No of Train Documents : \",len(document.readlines()))\n",
    "print(\"No of Train Summary : \",len(summ.readlines()))\n",
    "print(\"No of Train Files-name : \",len(file_name.readlines()))\n",
    "document.close()\n",
    "summ.close()\n",
    "file_name.close()\n",
    "\n",
    "# Validation Data Statistics\n",
    "document=open('val_document.txt','r')\n",
    "summ=open('val_title.txt','r')\n",
    "file_name=open('val_doc_name.txt','r')\n",
    "print(\"No of Val. Documents : \",len(document.readlines()))\n",
    "print(\"No of Val. Summary : \",len(summ.readlines()))\n",
    "print(\"No of Val. Files-name : \",len(file_name.readlines()))\n",
    "document.close()\n",
    "summ.close()\n",
    "file_name.close()\n",
    "\n",
    "# Test Data Statistics\n",
    "document=open('test_document.txt','r')\n",
    "summ=open('test_title.txt','r')\n",
    "file_name=open('test_doc_name.txt','r')\n",
    "print(\"No of test Documents : \",len(document.readlines()))\n",
    "print(\"No of test Summary : \",len(summ.readlines()))\n",
    "print(\"No of test Files-name : \",len(file_name.readlines()))\n",
    "document.close()\n",
    "summ.close()\n",
    "file_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
